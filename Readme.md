# **Lip Sync Model Implementation (Wav2Lip)**

## ğŸ“Œ **Objective**

This project implements the **Wav2Lip** model to generate lip-synced videos by synchronizing facial movements with speech. Given an **input video** and **audio file**, the model outputs a realistic video where the lips match the speech accurately.

## ğŸ— **Project Structure**

```
ğŸ“ LIPSYNC
 â”œâ”€â”€ ğŸ“‚ env/                 # Conda virtual environment
 â”œâ”€â”€ ğŸ“‚ input/               # Folder for input files
 â”‚   â”œâ”€â”€ input_video.mp4     # Input face video
 â”‚   â”œâ”€â”€ input.wav           # Input speech/audio
 â”‚   â”œâ”€â”€ Screenshot.png      # Screenshot (optional)
 â”‚
 â”œâ”€â”€ ğŸ“‚ output/              # Folder for generated lip-synced video
 â”‚   â”œâ”€â”€ result_voice.mp4    # Output video file
 â”‚
 â”œâ”€â”€ ğŸ“‚ Wav2Lip/             # Wav2Lip implementation files
 â”‚   â”œâ”€â”€ ğŸ“‚ checkpoints/     # Pretrained model files (to be added)
 â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/      # Evaluation scripts
 â”‚   â”œâ”€â”€ ğŸ“‚ face_detection/  # Face detection module
 â”‚   â”œâ”€â”€ ğŸ“‚ filelists/       # File list management
 â”‚   â”œâ”€â”€ ğŸ“‚ models/          # Model architecture files
 â”‚   â”œâ”€â”€ ğŸ“‚ results/         # Intermediate outputs
 â”‚   â”œâ”€â”€ ğŸ“‚ temp/            # Temporary processing files
 â”‚   â”œâ”€â”€ audio.py            # Audio processing script
 â”‚   â”œâ”€â”€ inference.py        # Main inference script
 â”‚   â”œâ”€â”€ preprocess.py       # Preprocessing script
 â”‚   â”œâ”€â”€ requirements.txt    # Dependency list
 â”‚   â”œâ”€â”€ wav2lip_train.py    # Training script (optional)
 â”‚   â”œâ”€â”€ README.md           # Wav2Lip documentation
 â”‚
 â”œâ”€â”€ Lip_Sync.ipynb          # Jupyter Notebook for implementation
 â”œâ”€â”€ Readme.md               # Project documentation
```

---

## ğŸ›  **Installation**

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Rudrabha/Wav2Lip.git
cd Wav2Lip
```

### 2ï¸âƒ£ Install Dependencies

Ensure you have Python 3.7+ installed. Then, install the required libraries:

```bash
pip install -r requirements.txt
```

ğŸ“Œ **Dependencies:**

- librosa==0.7.0
- numpy==1.17.1
- opencv-python==4.1.0.25
- torch==1.1.0
- torchvision==0.3.0
- tqdm==4.45.0
- numba==0.48

### 3ï¸âƒ£ Download Pretrained Model

The Wav2Lip model requires a pretrained checkpoint file. Download it from the following link:

ğŸ”— [Pretrained Model Download](https://drive.google.com/file/d/1xIMvN1w8bGUT7d9fdWwAJU4_cpGgHpu7/view?usp=drive_link)

After downloading, place the `.pth` file inside the `checkpoints/` directory.

---

## ğŸ¬ **Usage Instructions**

You can also run this project on Google Colab: [Colab Notebook](https://colab.research.google.com/drive/1FAD6Izn_KYaFxZe5xlXNW_sAYrrb7Lzq?usp=sharing)

### **Run via Jupyter Notebook**

1. Open `Lip_Sync.ipynb` in Jupyter Notebook.

### Step 1: Convert Text to Speech (TTS)

Convert the input text into a `.wav` file using a TTS engine such as gTTS:

```
output = gTTS(text=script, lang=language, slow=False)
output.save("/content/input.wav")
os.system("start /content/input.wav")
```

### Step 2: Convert Screenshot to Video

Generate a video from the input image:

```sh
import cv2


image = cv2.imread("/content/Screenshot.png")


fps = 30  # Frames per second

video_duration = 30  # Video duration in seconds


total_frames = int(fps * video_duration)

video_writer = cv2.VideoWriter("input_video.mp4", cv2.VideoWriter_fourcc('m','p','e','g'), fps, (image.shape[1], image.shape[0]))


for _ in range(total_frames):

    video_writer.write(image)


video_writer.release()

print("Video created successfully!")
```

### Step 3: Run Lip-Syncing Process

Run the Wav2Lip inference script to synchronize the image-based video with generated speech:

```sh
python Wav2Lip/inference.py --checkpoint_path Wav2Lip/checkpoints/wav2lip.pth --face input/input_video.mp4 --audio input/input.wav
```

This will generate a lip-synced video in the `output/` folder.

ğŸ“Œ The output file (`result_voice.mp4`) will be saved in the `output/` directory.

---

## ğŸ“Œ **Example Output**

**Input:**

- **Video:** A face speaking with no lip movement.
- **Audio:** Speech file (`input.wav`).

**Output:**

- **Generated video (`result_voice.mp4`)** with synchronized lip movements matching the input audio.

  ğŸ“¹ **Watch the Video**
[Download and Play](https://drive.google.com/file/d/1kh5Xn6jUquemaVINVGciqMJP3xujJpVx/view?usp=drive_link)

---

## ğŸ‘¤ **Contributor**

- **Sahil Kalal**

---
